{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from transformers import TFBertModel, TFBertForQuestionAnswering\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Raw SQuAD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = h5py.File(r'..\\SQuADv2\\train_386.h5', 'r')\n",
    "dev_data = h5py.File(r'..\\SQuADv2\\dev_386.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 386\n",
    "\n",
    "indices = np.arange(131911, dtype = int)\n",
    "shuffle = np.random.shuffle(indices)\n",
    "\n",
    "train_ids = np.array(train_data['input_ids'], dtype = np.int32)[indices]\n",
    "train_masks = np.array(train_data['attention_mask'], dtype = np.int32)[indices]\n",
    "train_tokens = np.array(train_data['token_type_ids'], dtype = np.int32)[indices]\n",
    "\n",
    "dev_ids = np.array(dev_data['input_ids'], dtype = np.int32)\n",
    "dev_masks = np.array(dev_data['attention_mask'], dtype = np.int32)\n",
    "dev_tokens = np.array(dev_data['token_type_ids'], dtype = np.int32)\n",
    "\n",
    "train_input_start = np.array(train_data['input_start'], dtype = np.int32)[indices]\n",
    "train_input_end = np.array(train_data['input_end'], dtype = np.int32)[indices]\n",
    "\n",
    "#dev_input_start = np.array(dev_data['input_start'], dtype = np.int32)\n",
    "#dev_input_end = np.array(dev_data['input_end'], dtype = np.int32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get BERT model with head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_bert_model():\n",
    "    max_seq_length = 386\n",
    "    bert_layer = TFBertModel.from_pretrained('bert-large-uncased')\n",
    "    \n",
    "    input_ids = Input((max_seq_length,), dtype = tf.int32, name = 'input_ids')\n",
    "    #input_segs = Input((512,), dtype = tf.int32)\n",
    "    input_masks = Input((max_seq_length,), dtype = tf.int32, name = 'input_masks')\n",
    "    input_tokens = Input((max_seq_length,), dtype = tf.int32, name = 'input_tokens')\n",
    "    pooled_outputs = bert_layer([input_ids, input_masks, input_tokens])[0] #1 for pooled outputs, 0 for sequence\n",
    "    logits = Dense(2)(pooled_outputs)\n",
    "    start_logits, end_logits = tf.split(logits, 2, axis=-1)\n",
    "    start_logits = tf.squeeze(start_logits, axis=-1)\n",
    "    end_logits = tf.squeeze(end_logits, axis=-1)\n",
    "\n",
    "    model = Model(inputs = [input_ids, input_masks, input_tokens], outputs = [start_logits, end_logits])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base = get_base_bert_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 386)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 386)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_tokens (InputLayer)       [(None, 386)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_bert_model (TFBertModel)     ((None, 386, 1024),  335141888   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 input_tokens[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 386, 2)       2050        tf_bert_model[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 386, 1), (No 0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [(None, 386)]        0           tf_op_layer_split[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_1 (TensorFl [(None, 386)]        0           tf_op_layer_split[0][1]          \n",
      "==================================================================================================\n",
      "Total params: 335,143,938\n",
      "Trainable params: 335,143,938\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(1e-5)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "bert_base.compile(loss = [loss_fn, loss_fn],\n",
    "                  optimizer=opt,\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial epoch fine-tuning for the first epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "3298/3298 [==============================] - 1377s 417ms/step - loss: 3.6843 - tf_op_layer_Squeeze_loss: 1.8727 - tf_op_layer_Squeeze_1_loss: 1.8116 - tf_op_layer_Squeeze_accuracy: 0.4664 - tf_op_layer_Squeeze_1_accuracy: 0.4789\n",
      "3298/3298 [==============================] - 1382s 419ms/step - loss: 2.5773 - tf_op_layer_Squeeze_loss: 1.3176 - tf_op_layer_Squeeze_1_loss: 1.2596 - tf_op_layer_Squeeze_accuracy: 0.5915 - tf_op_layer_Squeeze_1_accuracy: 0.6134\n",
      "3298/3298 [==============================] - 1376s 417ms/step - loss: 2.3405 - tf_op_layer_Squeeze_loss: 1.2022 - tf_op_layer_Squeeze_1_loss: 1.1383 - tf_op_layer_Squeeze_accuracy: 0.6283 - tf_op_layer_Squeeze_1_accuracy: 0.6551\n",
      "3298/3298 [==============================] - 1376s 417ms/step - loss: 2.1931 - tf_op_layer_Squeeze_loss: 1.1295 - tf_op_layer_Squeeze_1_loss: 1.0636 - tf_op_layer_Squeeze_accuracy: 0.6574 - tf_op_layer_Squeeze_1_accuracy: 0.6823\n",
      "3298/3298 [==============================] - 1375s 417ms/step - loss: 2.0507 - tf_op_layer_Squeeze_loss: 1.0724 - tf_op_layer_Squeeze_1_loss: 0.9782 - tf_op_layer_Squeeze_accuracy: 0.6722 - tf_op_layer_Squeeze_1_accuracy: 0.7011\n",
      "3298/3298 [==============================] - 1376s 417ms/step - loss: 1.9925 - tf_op_layer_Squeeze_loss: 1.0383 - tf_op_layer_Squeeze_1_loss: 0.9542 - tf_op_layer_Squeeze_accuracy: 0.6802 - tf_op_layer_Squeeze_1_accuracy: 0.7081\n",
      "3298/3298 [==============================] - 1375s 417ms/step - loss: 1.8840 - tf_op_layer_Squeeze_loss: 0.9772 - tf_op_layer_Squeeze_1_loss: 0.9068 - tf_op_layer_Squeeze_accuracy: 0.6992 - tf_op_layer_Squeeze_1_accuracy: 0.7208\n",
      "3298/3298 [==============================] - 1376s 417ms/step - loss: 1.8839 - tf_op_layer_Squeeze_loss: 0.9760 - tf_op_layer_Squeeze_1_loss: 0.9080 - tf_op_layer_Squeeze_accuracy: 0.7018 - tf_op_layer_Squeeze_1_accuracy: 0.7221\n",
      "3298/3298 [==============================] - 1376s 417ms/step - loss: 1.8066 - tf_op_layer_Squeeze_loss: 0.9301 - tf_op_layer_Squeeze_1_loss: 0.8765 - tf_op_layer_Squeeze_accuracy: 0.7149 - tf_op_layer_Squeeze_1_accuracy: 0.7348\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "3298/3298 [==============================] - 1317s 399ms/step - loss: 1.7741 - tf_op_layer_Squeeze_loss: 0.9232 - tf_op_layer_Squeeze_1_loss: 0.8509 - tf_op_layer_Squeeze_accuracy: 0.7175 - tf_op_layer_Squeeze_1_accuracy: 0.7409\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    if i == 9:\n",
    "        idx = indices[i*len(indices)//10:]\n",
    "    else:\n",
    "        idx = indices[i*len(indices)//10:(i+1)*len(indices)//10]\n",
    "    bert_base.fit([train_ids[idx], train_masks[idx], train_tokens[idx]], \n",
    "                  [train_input_start[idx], train_input_end[idx]],\n",
    "                  #validation_data=[[dev_ids, dev_masks],\n",
    "                  #[dev_input_start, dev_input_end]],\n",
    "                  epochs = 1,\n",
    "                  batch_size = 4,\n",
    "                  shuffle = True)\n",
    "    bert_base.save_weights('bert_squadv2_span_detection_weights_epoch_0_first_%i.h5' %i)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning for 6 full epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
      "32978/32978 [==============================] - 13786s 418ms/step - loss: 2.2374 - tf_op_layer_Squeeze_2_loss: 1.1519 - tf_op_layer_Squeeze_3_loss: 1.0855 - tf_op_layer_Squeeze_2_accuracy: 0.6508 - tf_op_layer_Squeeze_3_accuracy: 0.6739\n",
      "32978/32978 [==============================] - 13783s 418ms/step - loss: 1.3426 - tf_op_layer_Squeeze_2_loss: 0.7043 - tf_op_layer_Squeeze_3_loss: 0.6383 - tf_op_layer_Squeeze_2_accuracy: 0.7765 - tf_op_layer_Squeeze_3_accuracy: 0.8017\n",
      "32978/32978 [==============================] - 13787s 418ms/step - loss: 0.9030 - tf_op_layer_Squeeze_2_loss: 0.4783 - tf_op_layer_Squeeze_3_loss: 0.4247 - tf_op_layer_Squeeze_2_accuracy: 0.8401 - tf_op_layer_Squeeze_3_accuracy: 0.8635\n",
      "32978/32978 [==============================] - 13789s 418ms/step - loss: 0.6327 - tf_op_layer_Squeeze_2_loss: 0.3380 - tf_op_layer_Squeeze_3_loss: 0.2948 - tf_op_layer_Squeeze_2_accuracy: 0.8841 - tf_op_layer_Squeeze_3_accuracy: 0.9038\n",
      "32978/32978 [==============================] - 13790s 418ms/step - loss: 0.4628 - tf_op_layer_Squeeze_2_loss: 0.2481 - tf_op_layer_Squeeze_3_loss: 0.2146 - tf_op_layer_Squeeze_2_accuracy: 0.9153 - tf_op_layer_Squeeze_3_accuracy: 0.9288\n",
      "32978/32978 [==============================] - 13790s 418ms/step - loss: 0.3650 - tf_op_layer_Squeeze_2_loss: 0.1938 - tf_op_layer_Squeeze_3_loss: 0.1712 - tf_op_layer_Squeeze_2_accuracy: 0.9337 - tf_op_layer_Squeeze_3_accuracy: 0.9430\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    bert_base.fit([train_ids, train_masks, train_tokens], \n",
    "                  [train_input_start, train_input_end],\n",
    "                  #validation_data=[[dev_ids, dev_masks],\n",
    "                  #[dev_input_start, dev_input_end]],\n",
    "                  epochs = 1,\n",
    "                  batch_size = 4)\n",
    "    bert_base.save_weights('bert_squadv2_span_detection_weights_epoch_%d.h5' %i)\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Extracting the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.data.processors.squad import SquadV2Processor\n",
    "from transformers.data.processors.squad import squad_convert_examples_to_features\n",
    "from transformers import BertConfig\n",
    "from BERTVision.utils.evaluation import Squad2Config\n",
    "\n",
    "class UntrainedBertSquad2Faster(object):\n",
    "    def __init__(self, weights,\n",
    "                 config = Squad2Config()):\n",
    "        self.weights = weights\n",
    "        self.tokenizer = config.tokenizer\n",
    "        self.named_model = config.named_model\n",
    "        self.model = self.bert_large_uncased_for_squad2(config.max_seq_length)\n",
    "\n",
    "    def bert_large_uncased_for_squad2(self, max_seq_length):\n",
    "        input_ids = Input((max_seq_length,), dtype = tf.int32, name = 'input_ids')\n",
    "        input_masks = Input((max_seq_length,), dtype = tf.int32, name = 'input_masks')\n",
    "        input_tokens = Input((max_seq_length,), dtype = tf.int32, name = 'input_tokens')\n",
    "        \n",
    "        #Load model from huggingface\n",
    "        config = BertConfig.from_pretrained(\"bert-large-uncased\", output_hidden_states=True)\n",
    "        bert_layer = TFBertModel.from_pretrained(self.named_model, config = config)\n",
    "        bert_layer.load_weights(self.weights)\n",
    "\n",
    "        outputs, _, embeddings = bert_layer([input_ids, input_masks, input_tokens]) #1 for pooled outputs, 0 for sequence\n",
    "\n",
    "        model = Model(inputs = [input_ids, input_masks, input_tokens], outputs = [embeddings, outputs])\n",
    "        return model\n",
    "\n",
    "#weights for just encoders without the head\n",
    "#BERT Weights files are too large to store in github\n",
    "model = UntrainedBertSquad2Faster('bert_squadv2_span_detection_weights_epoch_1_BERT_ONLY.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get train data as well\n",
    "train_data = h5py.File('../SQuADv2/train_386.h5', 'r')\n",
    "train_ids = np.array(train_data['input_ids'], dtype = np.int32)\n",
    "train_masks = np.array(train_data['attention_mask'], dtype = np.int32)\n",
    "train_tokens = np.array(train_data['token_type_ids'], dtype = np.int32)\n",
    "train_input_start = np.array(train_data['input_start'], dtype = np.int32)\n",
    "train_input_end = np.array(train_data['input_end'], dtype = np.int32)\n",
    "train_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write in batches of 8 each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training data NO padding batches of 8\n",
    "data_dir = '../data/train_bert_1_epoch_fine_tuned_full386/'\n",
    "def write_file(directory, idx, embeddings):\n",
    "    with h5py.File(directory + str(idx) + '.h5', 'w') as f:\n",
    "        f.create_dataset('hidden_state_activations', data = embeddings)\n",
    "        \n",
    "embeddings = np.zeros((8,386,1024,25), dtype = np.float16)\n",
    "for i in range(12697,16489):\n",
    "    e, _ = model.model.predict([train_ids[i*8:(i+1)*8], train_masks[i*8:(i+1)*8], train_tokens[i*8:(i+1)*8]])\n",
    "    for j in range(25):\n",
    "        embeddings[:, :, :, j] = e[j]\n",
    "    \n",
    "    if e[0].shape[0] == 8:\n",
    "        write_file(data_dir, i*8, embeddings)\n",
    "    else:\n",
    "        write_file(data_dir', i*8, embeddings[:e[0].shape[0]])\n",
    "    if not i%1000:\n",
    "        print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
