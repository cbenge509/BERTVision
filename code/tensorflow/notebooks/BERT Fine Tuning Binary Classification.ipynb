{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from transformers import TFBertModel, TFBertForQuestionAnswering\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Activation, GlobalMaxPool1D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import multi_gpu_model, to_categorical\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from transformers.data.processors.squad import SquadV2Processor\n",
    "from transformers import BertTokenizer\n",
    "from transformers.data.processors.squad import squad_convert_examples_to_features\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tqdm.notebook import tqdm, tnrange\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = h5py.File(r'C:/w266/cris/BERTVision/data/squad_train.h5', 'r')\n",
    "dev_data = h5py.File(r'C:/w266/cris/BERTVision/data/squad_dev.h5', 'r')\n",
    "train_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.array(eval(open('indices.txt', 'r').readline()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 386\n",
    "\n",
    "#indices = np.arange(len(indices), dtype = int)\n",
    "#shuffle = np.random.shuffle(indices)\n",
    "\n",
    "train_ids = np.array(train_data['input_ids'], dtype = np.int32)[indices]\n",
    "train_masks = np.array(train_data['attention_mask'], dtype = np.int32)[indices]\n",
    "train_tokens = np.array(train_data['token_type_ids'], dtype = np.int32)[indices]\n",
    "\n",
    "dev_ids = np.array(dev_data['input_ids'], dtype = np.int32)\n",
    "dev_masks = np.array(dev_data['attention_mask'], dtype = np.int32)\n",
    "dev_tokens = np.array(dev_data['token_type_ids'], dtype = np.int32)\n",
    "\n",
    "train_input_start = np.array(train_data['input_start'], dtype = np.int32)[indices]\n",
    "train_input_end = np.array(train_data['input_end'], dtype = np.int32)[indices]\n",
    "\n",
    "answer_no_answer = np.where(train_input_start + train_input_end > 0, 0, 1)\n",
    "answer_no_answer = to_categorical(answer_no_answer).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SquadV2Processor()\n",
    "data_raw = processor.get_dev_examples(\"C:/w266/cris/BERTVision/data\")\n",
    "\n",
    "dev_answers = dict(zip([d.qas_id for d in data_raw], \n",
    "    [np.uint8(d.is_impossible) for d in data_raw]))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "dd_raw = squad_convert_examples_to_features(\n",
    "            examples = data_raw,\n",
    "            tokenizer = tokenizer,\n",
    "            max_seq_length = 386,\n",
    "            doc_stride = 128,\n",
    "            max_query_length = 64,\n",
    "            is_training = False,)\n",
    "\n",
    "dev_predict_qasids = [d.qas_id for d in dd_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./data/dev_qasids.pkl', 'wb') as handle:\n",
    "    pickle.dump(dev_predict_qasids, handle, protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_bert_model():\n",
    "    max_seq_length = 386\n",
    "    \n",
    "    bert_layer = TFBertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "    input_ids = Input((max_seq_length,), dtype = tf.int32, name = 'input_ids')\n",
    "    input_masks = Input((max_seq_length,), dtype = tf.int32, name = 'input_masks')\n",
    "    input_tokens = Input((max_seq_length,), dtype = tf.int32, name = 'input_tokens')\n",
    "\n",
    "    #1 for pooled outputs (CLS token), 0 for sequence\n",
    "    cls_output = bert_layer([input_ids, input_masks, input_tokens])[1]\n",
    "    x = Dense(2, name = 'dense_2', kernel_initializer = 'he_normal') (cls_output)\n",
    "\n",
    "    model = Model(inputs = [input_ids, input_masks, input_tokens], outputs = x, name = 'BERT_SQuADv2_BinaryClassification')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base = get_base_bert_model()\n",
    "print(bert_base.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(1e-5)\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "bert_base.compile(loss = [loss_fn],\n",
    "                  optimizer = opt,\n",
    "                  metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train BERT and store weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size, epochs, intervals = 4, 6, 10\n",
    "indices = range(len(train_ids))\n",
    "\n",
    "for i in range(epochs):\n",
    "    if i == 0:\n",
    "        for b in range(intervals):\n",
    "            if b == (intervals-1):\n",
    "                idx = indices[b * len(train_ids) // intervals:]\n",
    "                fname = './weights/bert_squadv2_binary_classification_weights_epoch_1.h5'\n",
    "            else:\n",
    "                idx = indices[b * len(train_ids) // intervals: (b+1) * len(train_ids) // intervals]\n",
    "                fname = './weights/bert_squadv2_binary_classification_weights_epoch_0__%d_tenth.h5' % (b+1)\n",
    "                \n",
    "            history = bert_base.fit(x = [train_ids[idx], train_masks[idx], train_tokens[idx]], y = answer_no_answer[idx],\n",
    "                epochs = 1, batch_size = batch_size, verbose = True, shuffle = True)\n",
    "            print(f\"\\nSaving `{fname}`...\\n\")\n",
    "            bert_base.save_weights(fname)\n",
    "    else:\n",
    "        history = bert_base.fit(x = [train_ids, train_masks, train_tokens], y = answer_no_answer,\n",
    "            epochs = 1, batch_size = batch_size, shuffle = True, verbose = True)\n",
    "        bert_base.save_weights('./weights/bert_squadv2_binary_classification_weights_epoch_%d.h5' % (i + 1))\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Against Dev, Capture F1/EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_fname = './weights/bert_squadv2_binary_classification_weights_epoch_'\n",
    "epoch_list = list(np.arange(0.1, 1.0, 0.1).astype(np.float16))\n",
    "epoch_list.extend(list(np.arange(1, 7, 1).astype(np.uint8)))\n",
    "file_list = [str(i).replace('.', '__') for i in epoch_list]\n",
    "for i, s in enumerate(file_list):\n",
    "    if (i+1) < intervals:\n",
    "        s = \"\".join([s, \"_tenth.h5\"])\n",
    "    else:\n",
    "        s = \"\".join([s, \".h5\"])\n",
    "    file_list[i] = \"\".join([base_fname, s])\n",
    "\n",
    "# add a round for the untuned BERT\n",
    "epoch_list = [0] + epoch_list\n",
    "file_list = ['n/a'] + file_list\n",
    "\n",
    "results = {'epoch':[], 'f1':[], 'em':[]}\n",
    "for e, f in tqdm(zip(epoch_list, file_list)):\n",
    "    print(f\"Predicting DEV results for epoch [{str(e)}] from file '{f}'...\")\n",
    "    if f == 'n/a':\n",
    "        bert_base = get_base_bert_model()\n",
    "    else:\n",
    "        bert_base.load_weights(f)\n",
    "\n",
    "    pred = bert_base.predict([dev_ids, dev_masks, dev_tokens])\n",
    "    pred = np.argmax(pred, axis = 1).astype(np.uint8)\n",
    "\n",
    "    df = pd.DataFrame({'qas_id':dev_predict_qasids, 'prediction':pred}).groupby(by='qas_id').agg({'prediction':'max'})\n",
    "    ans = pd.DataFrame(dev_answers, index =[0]).T\n",
    "    ans.columns = ['answer']\n",
    "    df = df.merge(ans, how='inner', left_index = True, right_index = True)\n",
    "\n",
    "    f1 = f1_score(y_true = df.answer.values, y_pred = df.prediction.values)\n",
    "    em = accuracy_score(y_true = df.answer.values, y_pred = df.prediction.values)\n",
    "    \n",
    "    results['epoch'].append(e)\n",
    "    results['f1'].append(f1)\n",
    "    results['em'].append(em)\n",
    "    \n",
    "    print(f\"epoch [{e}] f1 score: {f1}\")\n",
    "    print(f\"epoch [{e}] accuracy: {em}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(results).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
