\section{Conclusion and future work}

In this paper, we propose a parameter-efficient approach that achieves maximal BERT performance for QA span annotation and greatly reduces fine-tuning time required for BERT. Our models are trained on BERT hidden state activations (embeddings), and consistently outperform BERT at the same level of fine-tuning. By using an ensemble of our model with BERTâ€™s predictions, we further surpass BERT performance, reducing the need for fine-tuning by 1 epoch. We achieved similarly promising results for QA classification, which suggests that this approach works well with both the full sequence BERT embeddings and with the CLS token embedding. Future work might look at reducing fine-tuning even further, by focusing on modeling, or alternative approaches for combining BERT embeddings. Better data caching strategies could also help the practical application of this method in production, as data loading from disk can be slow for generic hardware. While this work focused on SQuAD 2.0, further work can also evaluate this method on other NLP tasks, such as GLUE. Success here would demonstrate the ability of the method to generalize beyond QA, including for advanced inference tasks, such as entailment and paraphrasing. Overall, we demonstrated as a proof-of-concept that BERT embeddings carries valuable information that can be leveraged for inference, in a way that reduces BERT fine-tuning and exceed BERT performance.