\section{Introduction}

% Tenney1 = tenney-etal-2019-bert
% Tenney2 = DBLP:journals/corr/abs-1905-06316
% Zhu = Zhu2020IncorporatingBI
% Chen = Chen_2020
% Aken = van_Aken_2019
% Vaswani = Vaswani2017
% Devlin = Devlin2019
% distil = sanh2019distilbert
% BLEU = Papineni02bleu:a
% Ma = ma2019universal

The introduction of Transformers \cite{Vaswani2017} has significantly advanced the state-of-the-art for many NLP tasks. The most well-known Transformer-based model is BERT \cite{Devlin2019}. The standard way to use BERT on a specific task is to first download pre-trained weights for the model, then fine-tune these weights on a supervised dataset. However, this procedure can be quite slow, and at times prohibitive for those without a powerful GPU/TPU, or those with limited CPU capacity. Smaller Transformers, such as DistilBERT \cite{sanh2019distilbert}, can fine-tune up to 60\% faster. However, such models tend to consistently underperform full-size BERT on a wide range of tasks. A method that reduces fine-tuning but maintains the same or better performance would make BERT more accessible for practical applications.

To develop our method, we draw inspiration from previous works that use BERT for feature extraction rather than for fine-tuning \citep{Zhu2020IncorporatingBI, Chen_2020}. For example, Zhu et al. showed that the sequence outputs from the final BERT layer can be used as contextualized embeddings to supplement the self-attention mechanism in an encoder/decoder neural translation model. This led to an improvement over the standard Transformer model in all tested language pairs on standard metrics (BLEU score \cite{Papineni02bleu:a}).

One characteristic these studies share with typical BERT inference is that only information from the final layer of BERT is used. However, a study by \cite{tenney-etal-2019-bert} suggests that all layers of BERT carry unique information. By training a series of classifiers within the edge probing framework \cite{DBLP:journals/corr/abs-1905-06316}, the authors computed how much each layer changes performance on eight labeling tasks, and the expected layer at which the model predicts the correct labels. The discovery is that syntactic information is encoded earlier in the network, while higher level semantic information comes later. Furthermore, classifier performance generally increases for all tasks when more layers are accounted for, starting from layer 0, suggesting that useful information is being incorporated at each progressive layer. Others, such as \cite{van_Aken_2019}, looked specifically at QA with SQuAD and published similar findings. Their work suggests that different layers encode different information important for QA.

\cite{ma2019universal} showed that a simple averaging of only the first and last layers of BERT results in contextualized embeddings that performed better than only using the final layer. The authors evaluated this approach on a variety of tasks such as text classification and question-answering. Together, these works suggest that the hidden state activations within BERT may contain unique information that can be used to augment the final layer. That said, the exact way of doing this requires further exploration.

In this work, we leverage the findings by Tenney and Ma as inspiration for developing a solution with a two-fold goal: $1.$ Reduce expensive BERT fine-tuning, and $2.$ Maintain or exceed BERT-level performance in the process. To do so, we extract the information-rich hidden states from each encoder layer and use the full embeddings as training data. We demonstrate that, for two question-answering tasks, even our simple architectures can match BERT performance at a fraction of the fine-tuning cost. Our best model saves on one full epoch of fine-tuning, and performs better than BERT, suggesting that our approach may be a desirable substitute to fine-tuning until convergence.
