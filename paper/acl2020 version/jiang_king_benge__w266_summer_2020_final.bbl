\begin{thebibliography}{5}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova}]{Devlin2019}
Jacob Devlin, Ming~Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock \href {http://arxiv.org/abs/1810.04805} {{BERT: Pre-training of deep
  bidirectional transformers for language understanding}}.
\newblock In \emph{NAACL HLT 2019 - 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies - Proceedings of the Conference}.

\bibitem[{Kim et~al.(2016)Kim, Jernite, Sontag, and Rush}]{Kim2016}
Yoon Kim, Yacine Jernite, David Sontag, and Alexander~M. Rush. 2016.
\newblock \href {http://arxiv.org/abs/1508.06615} {{Character-Aware neural
  language models}}.
\newblock In \emph{30th AAAI Conference on Artificial Intelligence, AAAI 2016}.

\bibitem[{Limaye et~al.(2019)Limaye, Pandit, and Vinay}]{Limaye2019}
Girish Limaye, Manish Pandit, and Sawal Vinay. 2019.
\newblock \href
  {https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15783457.pdf}
  {{BertNet: Combining BERT language representation with Attention and CNN for
  Reading Comprehension}}.

\bibitem[{Ma et~al.(2019)Ma, Wang, Ng, Nallapati, and Xiang}]{ma2019universal}
Xiaofei Ma, Zhiguo Wang, Patrick Ng, Ramesh Nallapati, and Bing Xiang. 2019.
\newblock \href {http://arxiv.org/abs/1910.07973} {Universal text
  representation from bert: An empirical study}.

\bibitem[{Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang}]{Rajpurkar2016}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.
\newblock {SQuad: 100,000+ questions for machine comprehension of text}.
\newblock In \emph{EMNLP 2016 - Conference on Empirical Methods in Natural
  Language Processing, Proceedings}.

\end{thebibliography}
