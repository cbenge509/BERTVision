\begin{thebibliography}{14}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{van Aken et~al.(2019)van Aken, Winter, Löser, and
  Gers}]{van_Aken_2019}
Betty van Aken, Benjamin Winter, Alexander Löser, and Felix~A. Gers. 2019.
\newblock \href {https://doi.org/10.1145/3357384.3358028} {How does bert answer
  questions?}
\newblock \emph{Proceedings of the 28th ACM International Conference on
  Information and Knowledge Management}.

\bibitem[{Chen et~al.(2020)Chen, Trabelsi, Heflin, Xu, and Davison}]{Chen_2020}
Zhiyu Chen, Mohamed Trabelsi, Jeff Heflin, Yinan Xu, and Brian~D. Davison.
  2020.
\newblock \href {https://doi.org/10.1145/3397271.3401044} {Table search using a
  deep contextualized language model}.
\newblock \emph{Proceedings of the 43rd International ACM SIGIR Conference on
  Research and Development in Information Retrieval}.

\bibitem[{Chollet(2016)}]{DBLP:journals/corr/Chollet16a}
Fran{\c{c}}ois Chollet. 2016.
\newblock \href {http://arxiv.org/abs/1610.02357} {Xception: Deep learning with
  depthwise separable convolutions}.
\newblock \emph{CoRR}, abs/1610.02357.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova}]{Devlin2019}
Jacob Devlin, Ming~Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock \href {http://arxiv.org/abs/1810.04805} {{BERT: Pre-training of deep
  bidirectional transformers for language understanding}}.
\newblock In \emph{NAACL HLT 2019 - 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies - Proceedings of the Conference}.

\bibitem[{Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  de~Laroussilhe, Gesmundo, Attariyan, and
  Gelly}]{DBLP:journals/corr/abs-1902-00751}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  de~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
\newblock \href {http://arxiv.org/abs/1902.00751} {Parameter-efficient transfer
  learning for {NLP}}.
\newblock \emph{CoRR}, abs/1902.00751.

\bibitem[{Ma et~al.(2019)Ma, Wang, Ng, Nallapati, and Xiang}]{ma2019universal}
Xiaofei Ma, Zhiguo Wang, Patrick Ng, Ramesh Nallapati, and Bing Xiang. 2019.
\newblock \href {http://arxiv.org/abs/1910.07973} {Universal text
  representation from bert: An empirical study}.

\bibitem[{Papineni et~al.(2002)Papineni, Roukos, Ward, and jing
  Zhu}]{Papineni02bleu:a}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu. 2002.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock pages 311--318.

\bibitem[{Rajpurkar et~al.(2018)Rajpurkar, Jia, and
  Liang}]{DBLP:journals/corr/abs-1806-03822}
Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.
\newblock \href {http://arxiv.org/abs/1806.03822} {Know what you don't know:
  Unanswerable questions for squad}.
\newblock \emph{CoRR}, abs/1806.03822.

\bibitem[{Sanh et~al.(2019)Sanh, Debut, Chaumond, and
  Wolf}]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019.
\newblock \href {http://arxiv.org/abs/1910.01108} {Distilbert, a distilled
  version of bert: smaller, faster, cheaper and lighter}.

\bibitem[{Szegedy et~al.(2014)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich}]{DBLP:journals/corr/SzegedyLJSRAEVR14}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott~E. Reed,
  Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
  2014.
\newblock \href {http://arxiv.org/abs/1409.4842} {Going deeper with
  convolutions}.
\newblock \emph{CoRR}, abs/1409.4842.

\bibitem[{Tenney et~al.(2019{\natexlab{a}})Tenney, Das, and
  Pavlick}]{tenney-etal-2019-bert}
Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019{\natexlab{a}}.
\newblock \href {https://doi.org/10.18653/v1/P19-1452} {{BERT} rediscovers the
  classical {NLP} pipeline}.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 4593--4601, Florence, Italy.
  Association for Computational Linguistics.

\bibitem[{Tenney et~al.(2019{\natexlab{b}})Tenney, Xia, Chen, Wang, Poliak,
  McCoy, Kim, Durme, Bowman, Das, and
  Pavlick}]{DBLP:journals/corr/abs-1905-06316}
Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R.~Thomas McCoy,
  Najoung Kim, Benjamin~Van Durme, Samuel~R. Bowman, Dipanjan Das, and Ellie
  Pavlick. 2019{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/1905.06316} {What do you learn from
  context? probing for sentence structure in contextualized word
  representations}.
\newblock \emph{CoRR}, abs/1905.06316.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{Vaswani2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, {\L}ukasz Kaiser, and Illia Polosukhin. 2017.
\newblock {Attention is all you need}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Zhu et~al.(2020)Zhu, Xia, Wu, He, Qin, Zhou, Li, and
  Liu}]{Zhu2020IncorporatingBI}
Jinhua Zhu, Yingce Xia, Lijun Wu, Di~He, Tao Qin, Wengang Zhou, Houqiang Li,
  and Tie-Yan Liu. 2020.
\newblock Incorporating bert into neural machine translation.
\newblock \emph{ArXiv}, abs/2002.06823.

\end{thebibliography}
