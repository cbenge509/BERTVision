% =====================================================================================================
%
%  MAIN
%
% =====================================================================================================
\begingroup
\renewcommand{\cleardoublepage}{}
\renewcommand{\clearpage}{}
\chapter*{Project Abstract}\label{chap:abstract}
\addcontentsline{toc}{chapter}{Project Abstract}
%\renewcommand{\chapter}[2]{}%q

\lettrineabstract{We present BERT Vision, a highly parameter-efficient approach for NLP tasks that significantly reduces the need for extended BERT fine-tuning. Our compression method uses information from the hidden state activations of each BERT transformer layer, which is discarded during typical BERT inference. Our method aims to maintain maximal BERT performance at a fraction of the training time and GPU/TPU expense. Furthermore, we extend the utility of our compressed model architecture by evaluating the performance on transfer learning to a wider range of NLP tasks post-compression.}\footnote{\textit{BERTVision} - so named for our method of peering within BERT for the signal hidden therein.}
\footnote{See GitHub repository: \href{https://github.com/cbenge509/BERTVision}{BERTVision}}

%\doubleline

\nocite{*}
\printbibliography[title=Reading List]

%----------------------------------------------------------------------------------------
\endgroup