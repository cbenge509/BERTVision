@inproceedings{Lin2014,
abstract = {We propose a novel deep network structure called “Network In Network”(NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
booktitle = {2nd International Conference on Learning Representations, ICLR 2014 - Conference Track Proceedings},
title = {{Network in network}},
year = {2014}
}

@inproceedings{Tenney2020,
	abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.},
	archivePrefix = {arXiv},
	arxivId = {1905.05950},
	author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
	booktitle = {ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference},
	doi = {10.18653/v1/p19-1452},
	eprint = {1905.05950},
	isbn = {9781950737482},
	title = {{BERT rediscovers the classical NLP pipeline}},
	year = {2020}
}

@inproceedings{Kim2016,
abstract = {We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-Term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-The-Art despite having 60{\%} fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.},
archivePrefix = {arXiv},
arxivId = {1508.06615},
author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
booktitle = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
eprint = {1508.06615},
isbn = {9781577357605},
title = {{Character-Aware neural language models}},
year = {2016}
}

@inproceedings{Rajpurkar2016,
abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0{\%}, a significant improvement over a simple baseline (20{\%}). However, human performance (86.8{\%}) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com.},
author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
booktitle = {EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings},
isbn = {9781945626258},
title = {{SQuad: 100,000+ questions for machine comprehension of text}},
year = {2016}
}

@inproceedings{Devlin2019,
abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5{\%} (7.7{\%} point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
archivePrefix = {arXiv},
arxivId = {1810.04805},
author = {Devlin, Jacob and Chang, Ming Wei and Lee, Kenton and Toutanova, Kristina},
booktitle = {NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference},
eprint = {1810.04805},
isbn = {9781950737130},
title = {{BERT: Pre-training of deep bidirectional transformers for language understanding}},
year = {2019}
}

@inproceedings{Vaswani2017,
abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems},
issn = {10495258},
title = {{Attention is all you need}},
year = {2017}
}

@inproceedings{Conneau2017,
abstract = {The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with the depth: using up to 29 convolutional layers, we report improvements over the state-ofthe- Art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.},
author = {Conneau, Alexis and Schwenk, Holger and Cun, Yann Le and Barrault, L{\"{o}}c},
booktitle = {15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017 - Proceedings of Conference},
doi = {10.18653/v1/e17-1104},
isbn = {9781510838604},
title = {{Very deep convolutional networks for text classification}},
year = {2017}
}

@online{Kuefler2016,
abstract = {Computer vision has driven many of the greatest advances in convolutional neural networks, a model family that has found only limited use for natural language processing. The inception module of GoogleNet in particular attains high classification accuracy with few parameters. This project attempts to harness the insights of the inception module in a jointly convolutional and recurrent natural language model. A set of 18 networks built on the idea of combining convolution and recurrence are evaluated on a fine-grained (8 class) sentiment analysis task using the IMDB Large Movie Dataset.},
author = {Kuefler, Alex R.},
title = {{Merging Recurrence and Inception-Like Convolution for Sentiment Analysis}},
year = {2016},
school = {Stanford University},
url = {https://cs224d.stanford.edu/reports/akuefler.pdf},
urldate = {2020-06-17}
}

@online{Limaye2019,
abstract = {This paper presents BertNet, a high-performance question answering system utilizing BERT language representation combined with QANet inspired RNN-free attention and encoder layers which trains faster than baseline BiDAF model. We design the final layer of of our model to work with SQuAD 2.0 1 to effectively accommodate predictions on questions with no answers. This paper also demonstrates additional improvement to the performance by using back translation based data augmentation techniques. We used English-Arabic-English as back translation languages for increasing training data and show significant performance gain on "difficult" questions which the model otherwise doesn’t perform well on. On SQuAD 2.0 data-set, our single model trained with data augmentation produces an F1 Score of 77.15 on dev-set.},
author = {Limaye, Girish, and Pandit, Manish, and Vinay, Sawal},
title = {{BertNet: Combining BERT language representation with Attention and CNN for Reading Comprehension}},
year = {2019},
school = {Stanford University},
url = {https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15783457.pdf},
urldate = {2020-06-17}
}

@online{Takeuchi2019,
abstract = {Bidirectional Encoder Representations from Transformers (BERT) is the current state-of-the-art language model which pretrains deep bidirectional representations for a wide-range of tasks. We aim to finetune this model for the task of SQUAD 2.0 question-answering by incorporating various deep learning techniques. First, we utilized dot-product attention and Convolutional Neural Networks to generate overall representation layers. Next, we modified the existing loss function to penalize the BERT model for predicting an answer when there isn’t one. Finally, we increased the complexity of BERT model to be more specific to the questionanswering task by incorporating highway networks, additional transformer layers and Bidirectional Attention Flow (BiDAF). Although well-motivated, some of these approaches increase performance because they either introduced noise or overfitted the model. However, running the model through a highway network and tuning hyper-parameters did nonetheless generate small gains in performance.},
author = {Takeuchi, Danny, and Tran, Kevin},
title = {{Improving SQUAD 2.0 Performance using BERT + X}},
year = {2019},
school = {Stanford University},
url = {https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/reports/default/15737384.pdf},
urldate = {2020-06-17}
}

@misc{ramach2019standalone,
    title={Stand-Alone Self-Attention in Vision Models},
    author={Prajit Ramachandran and Niki Parmar and Ashish Vaswani and Irwan Bello and Anselm Levskaya and Jonathon Shlens},
    year={2019},
    eprint={1906.05909},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}